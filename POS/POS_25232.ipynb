{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From corpora: download \"Brown\", from models download \"tagsets\", \"averaged_perceptron_tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download() # corpora, brown, ptb, models tagsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must tokenize string first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T', 'NNP'), ('h', 'NN'), ('e', 'NN'), (' ', 'NNP'), ('m', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), (' ', 'NN'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), (' ', 'NNP'), ('t', 'NN'), ('h', 'NN'), ('e', 'NN'), (' ', 'VBZ'), ('a', 'DT'), ('p', 'NN'), ('p', 'NN'), ('l', 'NN'), ('e', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag,word_tokenize, help\n",
    "        \n",
    "print(pos_tag('The man ate the apple')) # treats every character as word/token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built in pos_tag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('boy', 'NN'), ('ate', 'VB'), ('the', 'DT'), ('delicious', 'JJ'), ('cake', 'NN')]\n",
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n"
     ]
    }
   ],
   "source": [
    "# 1)a\n",
    "\n",
    "print(pos_tag(word_tokenize('The boy ate the delicious cake')))\n",
    "\n",
    "# 1)b\n",
    "help.brown_tagset('DT') # Gets the description of a tag and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2,3]\n",
    "l2 = [3,4,5]\n",
    "\n",
    "l = l1+l2\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rhetoric', 'Farmville', 'fiction', 'District', 'Uvea', 'Supernatural', 'dividing', 'child', 'Often', 'Dracula', 'Freud', 'river', 'paper', 'coincidence', 'hour', 'miles', 'tragedy', 'language', 'purpose', 'November', 'Hurricane', 'television', 'Futuna', 'group', 'activity', 'radio', 'history', 'deals', 'Morales', 'causes', 'decision', 'events', 'work', 'Wallis', 'writing', 'Reality', 'perihelion', 'antagonist', 'climate', 'Nine', 'facts', 'circle', 'list', 'Joan', 'world', 'point', 'radiation', 'struggle', 'Prince', 'pity', 'Person', 'places', 'ability', 'Vishal', 'August', 'things', 'superego', 'system', 'Christabel', 'names', 'Cauca', 'danger', 'June', 'action', 'department', 's/he', 'id', 'Frankenstein', 'plot', 'acts', 'stage', 'confusion', 'repeat', 'types', 'cycles', 'aphelion', 'width', 'peoples', 'people', 'fictum', 'movies', 'opposite', 'Judah', 'Wuthering', 'storm', 'Freytag', 'psyche', 'One', 'dates', 'somebody', 'Brontë', 'year', 'Conflict', 'person', 'district', 'Pakistan', 'States', 'imagination', 'climax', 'Console', 'source', 'Self', 'Mary', 'time', 'All', 'counties', 'hurricane', 'Atlantic', 'hero', 'examples', 'philosophy', 'Jack', 'Cumberland', 'films', 'literature', 'problem', 'looks', 'Coleridge', 'Gilbert', 'Plot', 'Virginia', 'Earth', 'element', 'Machine/Technology', 'United', 'north', 'Bhardwaj', 'story', 'path', 'distance', 'play', 'town', 'space', 'depressions', 'May', 'mythos', 'term', 'Messala', 'name', 'Coigneux', 'audience', 'changes', 'cyclone', 'middle', 'Department', 'director', 'Emily', 'Tropical', 'theme', 'humans', 'Technology', 'non-fiction', 'enemy', 'Stoker', 'Ziarat', 'sequence', 'province', 'sun', 'theory', 'traditions', 'thoughts', 'Terminator', 'cause', 'Depression', 'library', 'Joyce', 'books', 'word', 'conflict', 'times', 'emotion', 'Milankovitch', 'Latin', 'importance', 'end', 'rollercoaster', 'beginning', 'Samuel', 'periods', 'heroes', 'municipality', 'drama', 'character', 'Fiction', 'Heights', 'fears', 'Fire', 'situation', 'minutes', 'reader', 'pattern', 'Sun', 'way', 'effect', 'forces', 'Aristotle', 'Gustav', 'emotions', 'sheet', 'fight', 'Island', 'part', 'base', 'varies', 'tilt', 'Fakaʻuvea', 'intelligence', 'music', 'characters', 'nature', 'type', 'Poetics', 'Nature', 'mind', 'protagonist', 'Balochistan', 'robot', 'thousand', 'movie', 'rotation', 'resolution', 'vs', 'Gorden', 'change', 'parts', 'spring', 'Hauts-de-France', 'Somme', 'exposition', 'example', 'stories', 'orbit', 'kind', 'line', 'Society', 'drama—more', 'series', 'Shelley', 'Edward', 'Colombia', 'Robot', 'days', 'creation', 'season', 'author', 'km', 'structure', 'force', 'Hindi', 'ideas', 'London', 'commune', 'Authie', 'France', 'Bram'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk import FreqDist\n",
    "from collections import defaultdict\n",
    "\n",
    "tagged = []\n",
    "\n",
    "for root, dirs, files in os.walk('datasets/simple-wiki/single-docs'):\n",
    "    for file in files[:10]:\n",
    "        with open(os.path.join(root,file)) as f:\n",
    "            tags = pos_tag(word_tokenize(f.read()))\n",
    "            tagged+=tags # Build a list of pairs of all words and their tags in first 50 files\n",
    "\n",
    "print(set([word for (word, tag) in tagged if tag[:2] == 'NN'])) # Print all words whose tags start with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "# test_dict['plays']+=1 # This gives an error when uncommented, key has not been intialized\n",
    "\n",
    "if 'plays' not in test_dict: # Using normal dict, must initalize key\n",
    "    test_dict['plays'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a default dict, no need to initialize key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_dict = defaultdict(int)\n",
    "test_dict['plays']+=1\n",
    "\n",
    "print(test_dict['plays'])\n",
    "print(test_dict['eats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of FreqDist objects, to count the parts of speech assigned to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VB', 3), ('NN', 2)]\n",
      "('VB', 3)\n",
      "VB\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "tags = defaultdict(lambda: FreqDist())\n",
    "\n",
    "tags['play']['VB']+=3\n",
    "tags['play']['NN']+=2\n",
    "tags['play']['DT']+=1\n",
    "\n",
    "print(tags['play'].most_common(2)) # Gets the 2 most common elements\n",
    "print(tags['play'].most_common(2)[0]) # Gets the first most common element, pair of format (tag,count)\n",
    "print(tags['play'].most_common(2)[0][0]) # Gets the tag of the first most common element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]\n",
      "Number of sents in brown corpus 57340\n",
      "Number of tokens in brown corpus 1161192\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# 3)a\n",
    "all_tagged = brown.tagged_sents()\n",
    "print(all_tagged[0:2]) # The first two sentences in the tagged corpus: List of a list of (word, tag) pairs\n",
    "print('Number of sents in brown corpus', len(all_tagged))\n",
    "print('Number of tokens in brown corpus', sum([len(sent) for sent in all_tagged]))\n",
    "\n",
    "# 3)b\n",
    "train = all_tagged[:50000] # First 50000 sentences for training\n",
    "test = all_tagged[50000:] # Rest for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the unigram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rexroth', 'NP'), ('bitch', 'NN'), ('unsuspecting', 'JJ'), ('dignitaries', 'NNS'), ('Tractor', 'NN'), ('petty', 'JJ'), ('sumac', 'NN'), ('Erhart', 'NP'), ('Peiping', 'NP'), ('groundless', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "# Example format for dictionary named \"words\"\n",
    "#{'play': {'vb':200, 'nn':100}, eat:{'vb':100}}\n",
    "\n",
    "# 3)c\n",
    "words = defaultdict(lambda: FreqDist())\n",
    "\n",
    "for sent in train: # Loop over all training sents\n",
    "    for (word, pos) in sent: # Loop over all words and their true parts of speech in each setence\n",
    "        words[word][pos]+=1 # This word has been tagged by this pos once\n",
    "\n",
    "# Dictionary of words (keys) and their most repeated part of speech (values)\n",
    "# 3) d\n",
    "sorted_words = {}\n",
    "\n",
    "for word in words:\n",
    "    sorted_words[word] = words[word].most_common(1)[0][0]\n",
    "    \n",
    "print(list(sorted_words.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) e,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model accuracy: 0.8835427798667458\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for sent in test:\n",
    "    for (word, pos) in sent:\n",
    "        # If word in test sentence is present in unigram dictionary\n",
    "        # AND if the predicted part of speech (sorted_words[word]) is equal to the true part of speech\n",
    "        # Increment the number of correct predictions by 1\n",
    "        if word in sorted_words and sorted_words[word] == pos:\n",
    "            correct +=1\n",
    "        total+=1 # Count the total number of words in the test set\n",
    "        \n",
    "print('Unigram model accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a DeafultTagger that predicts all parts of speech as nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1091925588759153\n",
      "[('The', 'NN'), ('boy', 'NN'), ('ate', 'NN'), ('the', 'NN'), ('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import DefaultTagger\n",
    "\n",
    "default_tagger = DefaultTagger('NN')\n",
    "print(default_tagger.evaluate(test))\n",
    "\n",
    "print(default_tagger.tag(word_tokenize('The boy ate the apple')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8835427798667458\n",
      "[('He', 'PPS'), ('watched', 'VBD'), ('the', 'AT'), ('play', 'VB')]\n",
      "[('The', 'AT'), ('kids', 'NNS'), ('play', 'VB'), ('in', 'IN'), ('the', 'AT'), ('garden', 'NN')]\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', 'NN')]\n",
      "[('Salah', None), ('scored', 'VBD'), ('the', 'AT'), ('goal', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger\n",
    "\n",
    "unigram_tagger = UnigramTagger(train)\n",
    "\n",
    "print(unigram_tagger.evaluate(test))\n",
    "\n",
    "print(unigram_tagger.tag(word_tokenize('He watched the play'))) # Tagged play as verb\n",
    "print(unigram_tagger.tag(word_tokenize('The kids play in the garden')))\n",
    "print(unigram_tagger.tag(word_tokenize('I saw a green spider')))\n",
    "print(unigram_tagger.tag(word_tokenize('Salah scored the goal'))) # Can't find salah in train sents, tagged as none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34665875057721485\n",
      "[('He', 'PPS'), ('watched', 'VBD'), ('the', 'AT'), ('play', 'NN')]\n",
      "[('The', 'AT'), ('kids', 'NNS'), ('play', 'VB'), ('in', 'IN'), ('the', 'AT'), ('garden', 'NN')]\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import BigramTagger\n",
    "\n",
    "bigram_tagger = BigramTagger(train)\n",
    "print(bigram_tagger.evaluate(test))\n",
    "\n",
    "print(bigram_tagger.tag(word_tokenize('He watched the play'))) # play correctly tagged as noun\n",
    "print(bigram_tagger.tag(word_tokenize('The kids play in the garden')))\n",
    "print(bigram_tagger.tag(word_tokenize('I saw a green spider'))) # can't find the bigram green spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff tagger, from Unigram to DefaultTagegr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8901230292235636\n",
      "[('Salah', 'NN'), ('scored', 'VBD'), ('the', 'AT'), ('goal', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "backoff_unigram_tagger = UnigramTagger(train, backoff=default_tagger)\n",
    "print(backoff_unigram_tagger.evaluate(test))\n",
    "# couldn't find salah in unigram, so backed off to noun\n",
    "print(backoff_unigram_tagger.tag(word_tokenize('Salah scored the goal'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Bigram to Unigram (which backs off to DefaultTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9117603403918464\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "backoff_bigram_tagger = BigramTagger(train, backoff = backoff_unigram_tagger)\n",
    "print(backoff_bigram_tagger.evaluate(test))\n",
    "print(backoff_bigram_tagger.tag(word_tokenize('I saw a green spider')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
